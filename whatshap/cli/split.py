"""
Split reads by haplotype.

Reads FASTQ/BAM file and a list of haplotype assignments (such as generated by 
whatshap haplotag --output-haplotag-list). Outputs one FASTQ/BAM per haplotype.
BAM mode is intended for unmapped BAMs (such as provided by PacBio).
"""
import logging
import os
import pysam
from collections import defaultdict, Counter
import itertools
from argparse import SUPPRESS
from xopen import xopen

from contextlib import ExitStack
from whatshap.utils import detect_file_format
from whatshap.timer import StageTimer

logger = logging.getLogger(__name__)


def add_arguments(parser):
    arg = parser.add_argument
    arg('--output-h1', default=None,
        help='Output file to write reads from Haplotype 1 to. Use ending .gz to '
        'create gzipped file.')
    arg('--output-h2', default=None,
        help='Output file to write reads from Haplotype 2 to. Use ending .gz to '
        'create gzipped file.')
    arg('--output-folder', default=None,
        help='Destination folder to print output files to.')
    arg('--output-untagged', default=None,
        help='Output file to write untagged reads to. Use ending .gz to '
        'create gzipped file.')
    arg('--add-untagged', default=False, action='store_true',
        help='Add reads without tag to both H1 and H2 output streams.')
    arg('--pigz', default=False, action='store_true',
        help='Use the pigz program for gzipping output.')
    arg('--only-largest-block', default=False, action='store_true',
        help='Only consider reads to be tagged if they belong to the largest '
        'phased block (in terms of read count) on their respective chromosome')
    arg('--discard-unknown-reads', default=False, action='store_true',
        help='Only check the haplotype of reads listed in the haplotag list file. '
            'Reads (read names) not contained in this file will be discarded. '
            'In the default case (= keep unknown reads), those reads would be '
            'considered untagged and end up in the respective output file. '
            'Please be sure that the read names match between the input FASTQ/BAM '
            'and the haplotag list file.')
    arg('--read-lengths-histogram', default=None,
        help='Output file to write read lengths histogram to in tab separated format.')
    arg('--split-by-cluster', default=False, action='store_true',
        help='Split by given (chromosome) clusters.')
    arg('reads_file', metavar='READS', help='Input FASTQ/BAM file with reads (fastq can be gzipped)')
    arg('list_file', metavar='LIST',
        help='Tab-separated list with (at least) two columns <readname> and <haplotype>, or at least '
            'three columns <readname>, <chromosome-direction cluster>, and <chromosome cluster> (can be gzipped). '
            'Currently, the two haplotypes have to be named H1 and H2 (or none), and clusters must be integer. '
            'For haplotype list file, the output of the "haplotag" command can be used alternatively (4 columns), '
            'and this is required for using the "--only-largest-block" option (need phaseset and chromosome info).')


def open_possibly_gzipped(filename, exit_stack, readwrite='r', pigz=False):
    """
    TODO: this should be simplified after the the utils::detect_file_format
    function has been extended for more file formats

    TODO: the implicit dependency to the external tool pigz for faster I/O
    should be replaced with proper multi-threaded and buffered writers
    for the FASTQ output

    :param filename:
    :param exit_stack:
    :param readwrite:
    :param pigz:
    :return:
    """
    if filename is None:
        # this case is used in initialize_io_files
        requested_file = None
    elif readwrite == 'r':
        if filename.endswith('.gz'):
            requested_file = exit_stack.enter_context(gzip.open(filename, 'rt'))
        else:
            requested_file = exit_stack.enter_context(open(filename, 'r'))
    elif readwrite == 'w':
        if filename.endswith('.gz'):
            if pigz:
                g = Popen(['pigz'], stdout=open(filename, 'w'), stdin=PIPE)
                requested_file = exit_stack.enter_context(g.stdin)
            else:
                requested_file = exit_stack.enter_context(gzip.open(filename, 'wt'))
        else:
            requested_file = exit_stack.enter_context(open(filename, 'w'))
    else:
        raise ValueError('Invalid file open mode (must be "r" or "w"): {}'.format(readwrite))
    return requested_file


def select_reads_in_largest_phased_blocks(block_sizes, block_to_readnames):
    """
    :param block_sizes:
    :param block_to_readnames:
    :return:
    """
    selected_reads = set()
    logger.info('Determining largest blocks/phasesets per chromosome')
    for chromosome, block_counts in block_sizes.items():
        block_name, reads_in_block = block_counts.most_common(1)[0]
        logger.info(
            'Chromosome: {} - Phaseset: {} - Tagged reads: {}'.format(
                chromosome, block_name, reads_in_block
            )
        )
        selected_reads = selected_reads.union(set(block_to_readnames[(chromosome, block_name)]))
    logger.info('Total number of haplo-tagged reads in all largest phased blocks: {}'.format(len(selected_reads)))
    return selected_reads


def process_split_list_file(splitlist, line_parser, haplotype_to_int, only_largest_blocks, discard_unknown_reads, split_by = 'haplotype'):
    """
    :param splitlist:
    :param line_parser:
    :param haplotype_to_int:
    :param only_largest_blocks:
    :param discard_unknown_reads:
    :return:
    """

    is_header = splitlist.readline().startswith('#')
    if not is_header:
        splitlist.seek(0)

    # needed to determine largest phased block
    block_sizes = defaultdict(Counter)
    # for later removal of reads not in largest phased block;
    # since this can grow quite a bit, only fill if needed
    blocks_to_readnames = defaultdict(set)

    # this set should not be too large given
    # that the haplotag list file contains only
    # a subset of the reads in the input FASTQ/BAM
    known_reads = set()

    readname_to_haplotype = defaultdict(int)
    total_reads = 0

    clusters = set()
    for line in splitlist:
        total_reads += 1
        if split_by == 'cluster':
            readname, haplo_name, _, _ = line_parser(line)
            clusters.add(haplo_name)
            readname_to_haplotype[readname] = haplo_name
            continue
        
        readname, haplo_name, phaseset, chromosome = line_parser(line)

        try:
            haplo_num = haplotype_to_int[haplo_name]
        except KeyError:
            logger.error('Mapping the haplotype name to the corresponding haplotype '
                        'number failed. Currently, the haplotype name in the haplotag '
                        'list file has to be one of: none, H1, H2. The value that triggered '
                        'the error was: {}'.format(haplo_name))
            raise
        if haplo_num == 0:
            if discard_unknown_reads:
                known_reads.add(readname)
            # Some "trickery" here:
            # Haplotype 0 means untagged;
            # the return value of a defaultdict(int)
            # is zero for unknown keys, so no need to store
            # anything unless "--discard-unknown-reads" is True,
            # in which case we need to know all read names
            continue
        readname_to_haplotype[readname] = haplo_num
        if only_largest_blocks:
            block_sizes[chromosome][phaseset] += 1
            blocks_to_readnames[(chromosome, phaseset)].add(readname)

    tagged_reads = len(readname_to_haplotype)
    untagged_reads = total_reads - tagged_reads
    logger.info('Total number of reads in haplotag list: {}'.format(total_reads))
    logger.info('Total number of haplo-tagged reads: {}'.format(tagged_reads))
    logger.info('Total number of untagged reads: {}'.format(untagged_reads))

    if discard_unknown_reads:
        known_reads = known_reads.union(set(readname_to_haplotype.keys()))
        num_known_reads = len(known_reads)
        assert total_reads == num_known_reads, \
            'Mismatch between total number of reads and known reads: {} vs {}'.format(total_reads, num_known_reads)

    if only_largest_blocks:
        selected_reads = select_reads_in_largest_phased_blocks(
            block_sizes,
            blocks_to_readnames
        )
        readname_to_haplotype = defaultdict(int, {k: readname_to_haplotype[k] for k in selected_reads})
        num_removed_reads = total_reads - len(readname_to_haplotype)
        logger.info('Number of reads removed / '
                    'reads not overlapping largest phased blocks: {}'.format(num_removed_reads))

    clusters = list(clusters)
    clusters.sort()
    
    return readname_to_haplotype, known_reads, clusters

#def process_clustering_file(file_name):
#    """
#    :param filename:
#    :return:
#    """
#    
#    readname_to_clust = dict()
#
#    with open(file_name) as f:
#        # skip header
#        next(f)
#
#        for line in f:
#            sp = line.split()
#            readname, clust = sp[0], int(sp[-1])
#            readname_to_clust[readname] = clust
#
#    return readname_to_clust


def _two_column_parser(line):
    cols = line.strip().split('\t')[:2]
    return cols[0], cols[1], None, None

def _three_column_parser(line):
    cols = line.strip().split('\t')
    return cols[0], int(cols[2]), None, None


def _four_column_parser(line):
    return line.strip().split('\t')[:4]


def _bam_iterator(bam_file):
    """
    :param bam_file:
    :return:
    """
    for record in bam_file:
        yield record.query_name, len(record.query_sequence), record


def _fastq_string_iterator(fastq_file):
    """
    Explicit casting to string because pysam does not seem to
    have a writer for FASTQ files - note that this relies
    on opening all compressed files in "text" mode
    (see open_possibly_gzipped)

    :param fastq_file:
    :return:
    """
    for record in fastq_file:
        yield record.name, len(record.sequence), str(record) + '\n'


def _fastq_binary_iterator(fastq_file):
    """
    This one just exists for the pigz dependency
    :param fastq_file:
    :return:
    """
    for record in fastq_file:
        yield record.name, len(record.sequence), (str(record) + '\n').encode('utf-8')


def gfa_file_iterator(gfa_file):
    """
    :param gfa_file:
    :return:
    """    
    for line in gfa_file:
        if line.strip() == "":
            return
        sp = line.split()
        nodes = [sp[1]]
        node_len = None
        if sp[0]=='L': # edge line
            nodes.append(sp[3])
        if sp[0]=='S':
            node_len = len(sp[2])
        yield nodes, node_len, line



def check_split_list_information(split_list_filename, exit_stack, split_by = 'haplotype'):
    """
    Check if the haplotag list file has at least 4 columns
    (assumed to be read name, haplotype, phaseset, chromosome),
    or at least 2 columns (as above). Fails if the haplotag file
    is not tab-separated. Return suitable parser for format

    :param split_list: Tab-separated file with at least 2 or 4 columns
    :param exit_stack:
    :param split_by "haplotype" for diploid genome or "cluster" including reads with integer clusters:
    :return:
    """    
    split_list = open_possibly_gzipped(split_list_filename, exit_stack)
    first_line = split_list.readline().strip()
    # rewind to make sure a header-less file is processed correctly
    split_list.seek(0)
    has_chrom_info = False
    
    if split_by == 'cluster':
        line_parser = _three_column_parser
        return split_list, has_chrom_info, line_parser
    
    try:
        _, _, _, _ = first_line.split('\t')[:4]
        line_parser = _four_column_parser
    except ValueError:
        try:
            _, _ = first_line.split('\t')[:2]
            line_parser = _two_column_parser
        except ValueError:
            raise ValueError('First line of haplotag list file does not have '
                            'at least 2 columns, or it is not tab-separated: {}'.format(first_line))
    else:
        has_chrom_info = True
    return split_list, has_chrom_info, line_parser


def initialize_io_files(reads_file, output_h1, output_h2, output_untagged, output_folder, use_pigz, split_by, clusters, exit_stack):
    """
    :param reads_file:
    :param output_h1:
    :param output_h2:
    :param output_untagged:
    :output_folder:
    :param use_pigz:
    :param exit_stack:
    :return:
    """
    potential_fastq_extensions = [
        'fastq',
        'fastq.gz',
        'fastq.gzip'
        'fq',
        'fq.gz'
        'fq.gzip'
    ]

    potential_fasta_extensions = [
        '.fasta',
        '.fa'
    ]

    input_format = detect_file_format(reads_file)
    if input_format is None:
        # TODO: this is a heuristic, need to extend utils::detect_file_format
        if any([reads_file.endswith(ext) for ext in potential_fastq_extensions]):
            input_format = 'FASTQ'
        elif any([reads_file.endswith(ext) for ext in potential_fasta_extensions]):
            input_format = 'FASTA'
        elif reads_file.endswith('gfa'):
            input_format = 'GFA'
        else:
            raise ValueError('Undetected file format for input reads. '
                            'Expecting BAM or FASTQ (gzipped): {}'.format(reads_file))
    elif input_format == 'BAM':
        pass
    elif input_format in ['VCF', 'CRAM']:
        raise ValueError('Input file format detected as: {} '
                        'Currently, only BAM and FASTQ is supported.')
    else:
        # this means somebody changed utils::detect_file_format w/o
        # checking for usage throughout the code
        raise ValueError('Unexpected file format for input reads: {} - '
                        'Expecting BAM or FASTQ (gzipped)'.format(input_format))
    
    output_file_names = [output_untagged, output_h1, output_h2]
    if output_folder == None:
        output_folder = os.path.dirname(reads_file)
    if split_by == 'cluster':
        output_prefix, ext = os.path.basename(reads_file).split('.')
        output_suffix = ['_'+str(x) for x in clusters]
        output_file_names = [os.path.join(output_folder, output_prefix+suffix+'.'+ext) for suffix in output_suffix]
        

    if input_format == 'BAM':
        input_reader = exit_stack.enter_context(
            pysam.AlignmentFile(
                reads_file,
                mode="rb",
                check_sq=False  # I guess this is needed for unaligned PacBio native files
            )
        )
        input_iter = _bam_iterator
        output_writers = dict()
        for hap, outfile in enumerate(output_file_names):
            output_writers[hap] = exit_stack.enter_context(
                pysam.AlignmentFile(
                    os.devnull if outfile is None else outfile,
                    mode="wb",
                    template=input_reader,
                )
            )

    elif input_format in ['FASTQ', 'FASTA', 'GFA']:
        if input_format == 'GFA':
            input_reader = exit_stack.enter_context(open(reads_file, 'r'))
        else:
            # raw or gzipped is both handled by PySam
            input_reader = exit_stack.enter_context(pysam.FastxFile(reads_file))

        input_mode = "wb"
        if not (reads_file.endswith('.gz') or reads_file.endswith('.gzip')):
            input_mode = "w"

        if input_format == 'GFA':
            input_iter = gfa_file_iterator
        elif use_pigz:
            input_iter = _fastq_binary_iterator
        else:
            input_iter = _fastq_string_iterator # for fasta or fastq
        output_writers = dict()
        for hap, outfile in enumerate(output_file_names):
            # TODO jump through a hoop here to not break pigz feature; should be
            # changed to WhatsHap-internal features
            open_handle = open_possibly_gzipped(outfile, exit_stack, "w", use_pigz)
            if open_handle is None:
                open_handle = exit_stack.enter_context(open(os.devnull, input_mode))
            output_idx = hap if split_by == 'haplotype' else hap+1
            output_writers[output_idx] = open_handle
    else:
        # and this means I overlooked something...
        raise ValueError('Unhandled file format for input reads: {}'.format(input_format))
    return input_reader, input_iter, output_writers


def write_read_length_histogram(length_counts, histogram_file, exit_stack):
    """
    :param length_counts:
    :param histogram_file:
    :param exit_stack:
    :return:
    """
    h1 = length_counts[1]
    h2 = length_counts[2]
    untag = length_counts[0]
    all_read_lengths = sorted(
        itertools.chain(
            *(h1.keys(), h2.keys(), untag.keys())
        )
    )
    tsv_file = open_possibly_gzipped(histogram_file, exit_stack, "w", pigz=False)
    _ = tsv_file.write('\t'.join(['#length', 'count-untagged', 'count-h1', 'count-h2']) + '\n')

    line = '{}\t{}\t{}\t{}'

    out_lines = [line.format(rlen, untag[rlen], h1[rlen], h2[rlen]) for rlen in all_read_lengths]

    _ = tsv_file.write('\n'.join(out_lines))

    return


def run_split(
        reads_file,
        list_file,
        output_h1=None,
        output_h2=None,
        output_untagged=None,
        output_folder=None,
        add_untagged=False,
        pigz=False,
        only_largest_block=False,
        discard_unknown_reads=False,
        read_lengths_histogram=None,
        split_by_cluster=False,
    ):

    timers = StageTimer()
    timers.start('split-run')

    with ExitStack() as stack:
        timers.start('split-init')
        
        split_by = 'cluster' if split_by_cluster else 'haplotype'
        
        # TODO: obviously this won't work for more than two haplotypes
        haplotype_to_int = {'none': 0, 'H1': 1, 'H2': 2}

        haplo_list, has_haplo_chrom_info, line_parser = check_split_list_information(
            list_file, stack, split_by)

        if only_largest_block:
            logger.debug('User selected "--only-largest-block", this requires chromosome '
                        'and phaseset information to be present in the haplotag list file.')
            if split_by=='haplotype' and not has_haplo_chrom_info:
                raise ValueError('The haplotag list file does not contain phaseset and chromosome '
                                'information, which is required to select only reads from the '
                                'largest phased block. Columns 3 and 4 are missing.')

        timers.start('split-process-haplotag-list')

        readname_to_haplotype, known_reads, clusters = process_split_list_file(
            haplo_list,
            line_parser,
            haplotype_to_int,
            only_largest_block,
            discard_unknown_reads,
            split_by
        )
        if discard_unknown_reads:
            logger.debug('User selected to discard unknown reads, i.e., ignore all reads '
                        'that are not part of the haplotag list input file.')
            assert len(known_reads) > 0, \
                'No known reads in input set - would discard everything, this is probably wrong'
            missing_reads = len(known_reads)
        else:
            missing_reads = -1

        timers.stop('split-process-haplotag-list')

        input_reader, input_iterator, output_writers = initialize_io_files(
            reads_file,
            output_h1,
            output_h2,
            output_untagged,
            output_folder,
            pigz,
            split_by,
            clusters,
            stack,
        )

        timers.stop('split-init')

        histogram_data = {
            0: Counter(),
            1: Counter(),
            2: Counter(),
        }

        # holds count statistics about total processed reads etc.
        read_counter = Counter()

        process_haplotype = {
            0: output_untagged is not None or add_untagged,
            1: output_h1 is not None,
            2: output_h2 is not None
        }

        timers.start('split-iter-input')
        
        for read_name, read_length, record in input_iterator(input_reader):
            if type(read_name) != list:
                read_name = [read_name]
            if read_length != None: # for skipping non-node lines in a gfa file
                read_counter['total_reads'] += 1

            if discard_unknown_reads and any([read not in known_reads for read in read_name]):
                if read_length != None:
                    read_counter['unknown_reads'] += 1
                continue
            read_haplotype = [readname_to_haplotype[read] for read in read_name]
            if len(set(read_haplotype)) > 1 and 0 not in read_haplotype:
                # the two ends of the edge have different not-none haplotypes/clusters
                continue

            read_haplotype = max(read_haplotype)

            if split_by == 'haplotype' and not process_haplotype[read_haplotype]:
                if read_length != None:
                    read_counter['skipped_reads'] += 1
                continue

            if split_by == 'haplotype' and read_length != None:
                histogram_data[read_haplotype][read_length] += 1
                read_counter[read_haplotype] += 1

            output_writers[read_haplotype].write(record)
            if split_by == 'haplotype' and read_haplotype == 0 and add_untagged:
                output_writers[1].write(record)
                output_writers[2].write(record)

            if discard_unknown_reads:
                missing_reads -= 1
                if missing_reads == 0:
                    logger.info('All known reads processed - cancel processing...')
                    break

        timers.stop('split-iter-input')

        if read_lengths_histogram is not None:
            timers.start('split-length-histogram')
            write_read_length_histogram(histogram_data, read_lengths_histogram, stack)
            timers.stop('split-length-histogram')

    timers.stop('split-run')

    logger.info('\n== SUMMARY ==')
    logger.info('Total reads processed: {}'.format(read_counter['total_reads']))
    logger.info('Number of output reads "untagged": {}'.format(read_counter[0]))
    logger.info('Number of output reads haplotype 1: {}'.format(read_counter[1]))
    logger.info('Number of output reads haplotype 2: {}'.format(read_counter[2]))
    logger.info('Number of unknown (dropped) reads: {}'.format(read_counter['unknown_reads']))
    logger.info('Number of skipped reads (per user request): {}'.format(read_counter['skipped_reads']))

    logger.info('Time for processing haplotag list: {} sec'.format(
        round(timers.elapsed('split-process-haplotag-list'), 3)))

    logger.info('Time for total initial setup: {} sec'.format(
        round(timers.elapsed('split-init'), 3)))

    logger.info('Time for iterating input reads: {} sec'.format(
        round(timers.elapsed('split-iter-input'), 3)))

    if read_lengths_histogram is not None:
        logger.info('Time for creating histogram output: {} sec'.format(
            round(timers.elapsed('split-length-histogram'), 3)))

    logger.info('Total run time: {} sec'.format(
        round(timers.elapsed('split-run'), 3)))

def main(args):
    run_split(**vars(args))
